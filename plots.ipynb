{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from ccs.files import ccs_reporter_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_results(paths):\n",
    "    \"\"\"\n",
    "    Load experiment results from given paths in the ccs_reporters_dir.\n",
    "    Can handle paths to both individual experiment directories and sweep directories.\n",
    "\n",
    "    Args:\n",
    "    paths (list of str): Relative paths in the ccs_reporters_dir.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with the experiment results.\n",
    "    \"\"\"\n",
    "    root_dir = ccs_reporter_dir().as_posix()\n",
    "    all_data = []\n",
    "\n",
    "    for path in paths:\n",
    "        full_path = os.path.join(root_dir, path)\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(full_path):\n",
    "            # TODO: skipping transfer results for now.\n",
    "            if \"transfer\" in dirpath:\n",
    "                continue\n",
    "            if \"eval.csv\" in filenames:\n",
    "                # Extract model and dataset from the path\n",
    "                parts = dirpath.replace(root_dir, '').strip('/').split('/')\n",
    "                dataset = parts[-1]\n",
    "                if len(parts) == 4:\n",
    "                    model = parts[-2]\n",
    "                elif len(parts) == 5:\n",
    "                    model = os.path.join(*parts[-3:-1])\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected path: {dirpath}\")\n",
    "                # print(f\"Loading {model} on {dataset}\")\n",
    "\n",
    "                # Load eval.csv\n",
    "                eval_df = pd.read_csv(os.path.join(dirpath, \"eval.csv\"))\n",
    "                eval_df.columns = ['eval_' + col for col in eval_df.columns]\n",
    "\n",
    "                # Load lr_eval.csv\n",
    "                lr_eval_df = pd.read_csv(os.path.join(dirpath, \"lr_eval.csv\"))\n",
    "                lr_eval_df.columns = ['lr_eval_' + col for col in lr_eval_df.columns]\n",
    "\n",
    "                # Load lm_eval.csv if exists\n",
    "                lm_eval_file = os.path.join(dirpath, \"lm_eval.csv\")\n",
    "                if os.path.exists(lm_eval_file):\n",
    "                    lm_eval_df = pd.read_csv(lm_eval_file)\n",
    "                    lm_eval_df.columns = ['lm_eval_' + col for col in lm_eval_df.columns]\n",
    "                else:\n",
    "                    lm_eval_df = pd.DataFrame(columns=['lm_eval_' + col for col in eval_df.columns])\n",
    "                    lm_eval_df.loc[0] = [pd.NA] * len(lm_eval_df.columns)\n",
    "\n",
    "                # Combine all dataframes\n",
    "                combined_df = pd.concat([eval_df, lr_eval_df, lm_eval_df], axis=1)\n",
    "                combined_df['model'] = model\n",
    "                combined_df['dataset'] = dataset\n",
    "                combined_df['path'] = dirpath.replace(root_dir, '').strip('/')\n",
    "\n",
    "                all_data.append(combined_df)\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"sweeps/fervent-heisenberg\",\n",
    "    \"sweeps/thirsty-wing\",\n",
    "    \"sweeps/gallant-davinci\",\n",
    "]\n",
    "results_df = load_experiment_results(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['microsoft/deberta-v2-xxlarge-mnli', 'EleutherAI/gpt-j-6B',\n",
       "       'gpt2-xl'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.model.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_by_model(df):\n",
    "    \"\"\"\n",
    "    Creates a figure with subplots for each model showing grouped bar plots\n",
    "    of the average accuracy of CCS, LR, and Zero-Shot for each dataset.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame with experiment results.\n",
    "    \"\"\"\n",
    "    # Filter relevant columns\n",
    "    df = df[\n",
    "        [\n",
    "            \"model\",\n",
    "            \"dataset\",\n",
    "            \"eval_cal_acc_estimate\",\n",
    "            \"lr_eval_cal_acc_estimate\",\n",
    "            \"lm_eval_cal_acc_estimate\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Group by model and dataset and calculate mean\n",
    "    grouped_df = df.groupby([\"model\", \"dataset\"]).mean().reset_index()\n",
    "\n",
    "    # Find unique models\n",
    "    models = grouped_df[\"model\"].unique()\n",
    "\n",
    "    # Set latex style for plots\n",
    "    # plt.style.use(\"seaborn-paper\")\n",
    "    plt.style.use(\"seaborn-v0_8-poster\")\n",
    "    plt.rc(\"text\", usetex=True)\n",
    "    plt.rc(\"font\", family=\"serif\")\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(\n",
    "        len(models), 1, figsize=(10, 5 * len(models)), sharex=True\n",
    "    )\n",
    "\n",
    "    if len(models) == 1:\n",
    "        axes = [axes]  # Ensure axes is always a list\n",
    "\n",
    "    for ax, model in zip(axes, models):\n",
    "        # Filter data for this model\n",
    "        model_df = grouped_df[grouped_df[\"model\"] == model]\n",
    "\n",
    "        # Plot grouped bar plot\n",
    "        sns.barplot(\n",
    "            data=model_df,\n",
    "            x=\"dataset\",\n",
    "            y=\"eval_cal_acc_estimate\",\n",
    "            color=\"blue\",\n",
    "            ax=ax,\n",
    "            label=\"CCS\",\n",
    "        )\n",
    "        sns.barplot(\n",
    "            data=model_df,\n",
    "            x=\"dataset\",\n",
    "            y=\"lr_eval_cal_acc_estimate\",\n",
    "            color=\"orange\",\n",
    "            ax=ax,\n",
    "            label=\"LR\",\n",
    "        )\n",
    "        sns.barplot(\n",
    "            data=model_df,\n",
    "            x=\"dataset\",\n",
    "            y=\"lm_eval_cal_acc_estimate\",\n",
    "            color=\"green\",\n",
    "            ax=ax,\n",
    "            label=\"Zero-Shot\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Model: {model}\")\n",
    "        ax.set_ylabel(\"Average Accuracy\")\n",
    "        ax.set_xlabel(\"\")\n",
    "\n",
    "    # Add legend\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", ncol=3)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/scratch/users/ebronstein/ccs/plots.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bshelob/scratch/users/ebronstein/ccs/plots.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m plot_accuracy_by_model(results_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "plot_accuracy_by_model(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eleuther-ccs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
